{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Kng Yew Chian, October 2023\n",
        "# Using pre-trained word embeddings and a BILSTM to classify each entity with the IO tagging scheme"
      ],
      "metadata": {
        "id": "qKWwDw9dJiOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR9kxHJXTvMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6262d102-ca4f-4d44-be3b-583e3f6b6556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "4R6u-cuFUGJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "HMaKQ8RyUP6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "metadata": {
        "id": "IvG5_Sv4UpYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae5c7d5-fa3e-43d6-c144-37b0e2e7b57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ./drive; to attempt to forcibly remount, call drive.mount(\"./drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('./drive/MyDrive/embeddings.pkl', 'wb') as file:\n",
        "#    pickle.dump(embeddings, file)"
      ],
      "metadata": {
        "id": "jaIQkm4B9jRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./drive/MyDrive/embeddings.pkl\", \"rb\") as f:\n",
        "  embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "pgmkjkM61p9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1.1 Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word and its cosine similarity."
      ],
      "metadata": {
        "id": "ou77FClyXaa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.most_similar('student')[0])\n",
        "print(embeddings.most_similar('Apple')[0])\n",
        "print(embeddings.most_similar('apple')[0])"
      ],
      "metadata": {
        "id": "_Sr8I7B7dAwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b006d3-decd-41c0-955c-0f5ce2a36717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('students', 0.7294867038726807)\n",
            "('Apple_AAPL', 0.7456986308097839)\n",
            "('apples', 0.720359742641449)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_embeddings = embeddings['student'].reshape(1,300)\n",
        "students_embeddings = embeddings['students'].reshape(1,300)\n",
        "print(metrics.pairwise.cosine_similarity(student_embeddings, students_embeddings))"
      ],
      "metadata": {
        "id": "qW5aHaucXpBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95548b1b-d8ab-4f9c-9b63-1adbb0f6d932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7294867]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "processing data"
      ],
      "metadata": {
        "id": "ukR9m-jK1MU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header_names = ['word', 'useless1', 'useless2', 'tag']\n",
        "train_raw = pd.read_csv(\"./drive/MyDrive/eng.train\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")\n",
        "validation_raw = pd.read_csv(\"./drive/MyDrive/eng.testa\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")\n",
        "test_raw = pd.read_csv(\"./drive/MyDrive/eng.testb\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")"
      ],
      "metadata": {
        "id": "-GaEs2V9XwWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reads \" as string instead of quote\n",
        "# train_raw[train_raw['word'] == '\"']"
      ],
      "metadata": {
        "id": "w1fG9LyA30Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_indices = train_raw[train_raw['word'].isnull()].reset_index()['index']"
      ],
      "metadata": {
        "id": "SugwY4N6hpUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_na_labels(data):\n",
        "  temp = data[['word', 'tag']]\n",
        "  #without_na = temp.dropna(subset=['tag']).reset_index().drop(columns='index')\n",
        "  return temp"
      ],
      "metadata": {
        "id": "wKl01oQbV0SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tag_to_integer_dict(train_without_na):\n",
        "  count = 0\n",
        "  tag_to_integer_dictionary = {}\n",
        "  for tag in train_without_na['tag'].unique():\n",
        "    tag_to_integer_dictionary[tag] = count\n",
        "    count += 1\n",
        "\n",
        "  return tag_to_integer_dictionary"
      ],
      "metadata": {
        "id": "H1SVCkLYV9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tag_to_integer_dict_from_raw(train_raw):\n",
        "  temp = train_raw[['word', 'tag']]\n",
        "  without_na = temp.dropna(subset=['tag']).reset_index().drop(columns='index')\n",
        "  tag_to_integer_dictionary = get_tag_to_integer_dict(without_na)\n",
        "  return tag_to_integer_dictionary"
      ],
      "metadata": {
        "id": "ejHvXn5Dahda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_to_integer_dictionary = get_tag_to_integer_dict_from_raw(train_raw)"
      ],
      "metadata": {
        "id": "SohFLRnJa3lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences_wordstags_array(train_without_na):\n",
        "  sentences_train = []\n",
        "  new_sentence = []\n",
        "\n",
        "  for i, row in train_without_na.iterrows():\n",
        "\n",
        "    if row.isna().all():\n",
        "      sentences_train.append(new_sentence)\n",
        "      new_sentence = []\n",
        "\n",
        "    else:\n",
        "      if len(new_sentence) > 30:\n",
        "        # if sentence length is too long, break it up every 30 words, to prevent timesteps from being too large\n",
        "        new_sentence.append([row[0], row[1]])\n",
        "        sentences_train.append(new_sentence)\n",
        "        new_sentence = []\n",
        "\n",
        "      else:\n",
        "        new_sentence.append([row[0], row[1]])\n",
        "\n",
        "\n",
        "#    if i > 10000:\n",
        "#      break\n",
        "\n",
        "  return sentences_train"
      ],
      "metadata": {
        "id": "ESmqVvGEjPsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences_embeddingstags_array(sentences_wordstags_array, tag_to_integer_dictionary):\n",
        "  # replace words with embeddings and tags with integers\n",
        "  embeddings_in_sentences = []\n",
        "  new_sentence = []\n",
        "\n",
        "  count = 0\n",
        "  for sentence in sentences_wordstags_array:\n",
        "    for word, tag in sentence:\n",
        "      if tag not in tag_to_integer_dictionary:\n",
        "        #print(f\"{tag} not found with {word}, skipping\")\n",
        "        continue\n",
        "      if word not in embeddings:\n",
        "        new_embedding = np.zeros(300)\n",
        "\n",
        "      else:\n",
        "        unnormalized_embedding = embeddings[word].astype(np.float32)\n",
        "        # Reshape the embedding to be a 2D array with a single row\n",
        "        embedding_reshaped = unnormalized_embedding.reshape(1, -1)\n",
        "        # Normalize the embedding\n",
        "        embedding_normalized = normalize(embedding_reshaped, axis=1, norm='l2')\n",
        "        # Flatten the normalized embedding back into a 1D array\n",
        "        new_embedding = embedding_normalized.flatten()\n",
        "\n",
        "      new_sentence.append([new_embedding, tag_to_integer_dictionary[tag]])\n",
        "      count += 1\n",
        "\n",
        "    embeddings_in_sentences.append(new_sentence)\n",
        "    new_sentence = []\n",
        "\n",
        "  return embeddings_in_sentences"
      ],
      "metadata": {
        "id": "ghMoFyPjg_U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_without_labels(sentences_embeddingstags):\n",
        "  # create copy that doesnt have the NER tag\n",
        "  without_label = []\n",
        "  new_sentence = []\n",
        "\n",
        "  count = 0\n",
        "  for sentence in sentences_embeddingstags:\n",
        "    for embedding, tag in sentence:\n",
        "      new_sentence.append(embedding)\n",
        "      count += 1\n",
        "\n",
        "    without_label.append(new_sentence)\n",
        "    new_sentence = []\n",
        "\n",
        "  return without_label"
      ],
      "metadata": {
        "id": "VHzWQ3hAx3uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(sentences_embeddingstags):\n",
        "  # create labels\n",
        "  labels = []\n",
        "  new_sentence = []\n",
        "\n",
        "  count = 0\n",
        "  for sentence in sentences_embeddingstags:\n",
        "    for embedding, tag in sentence:\n",
        "      new_sentence.append(tag)\n",
        "      count += 1\n",
        "\n",
        "    labels.append(np.array(new_sentence))\n",
        "    new_sentence = []\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "i9ojMMOHTeIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_without_labels_and_labels(without_labels, labels, max_sentence_length):\n",
        "  # padding\n",
        "  print(\"padding: \", max_sentence_length)\n",
        "  padded = tf.keras.utils.pad_sequences(without_labels, padding=\"post\", dtype=\"float32\", maxlen=max_sentence_length, value=0)\n",
        "  padded_labels = tf.keras.utils.pad_sequences(labels, padding=\"post\", maxlen=max_sentence_length, value=999)\n",
        "\n",
        "  return padded, padded_labels"
      ],
      "metadata": {
        "id": "b0XMqMRfwl21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_padded_and_padded_labels_to_np_arrays(padded, padded_labels):\n",
        "  # input: [batch, timestep, feature]\n",
        "  padded_np = np.array(padded)\n",
        "  padded_labels_np = np.array(padded_labels)\n",
        "  \"\"\"\n",
        "  print(type(train_labels))\n",
        "  print(type(train_labels[0]))\n",
        "  print(type(train_labels[0][0]))\n",
        "  print(type(train))\n",
        "  print(type(train[0]))\n",
        "  print(type(train[0][0]))\n",
        "  print(type(train[0][0][0]))\n",
        "  \"\"\"\n",
        "  return padded_np, padded_labels_np\n",
        "\n"
      ],
      "metadata": {
        "id": "SZWxQecY1tRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_raw_to_input_and_labels(raw, tag_to_integer_dictionary):\n",
        "  without_na = drop_na_labels(raw)\n",
        "  num_classes = len(tag_to_integer_dictionary)\n",
        "  sentences_wordstags = get_sentences_wordstags_array(without_na)\n",
        "  sentences_embeddingstags = get_sentences_embeddingstags_array(sentences_wordstags, tag_to_integer_dictionary)\n",
        "  without_labels = get_without_labels(sentences_embeddingstags)\n",
        "  labels = get_labels(sentences_embeddingstags)\n",
        "  \"\"\"\n",
        "  del without_na\n",
        "  del sentences_wordstags\n",
        "  del sentences_embeddingstags\n",
        "\n",
        "  gc.collect()\n",
        "  \"\"\"\n",
        "\n",
        "  return without_labels, labels, tag_to_integer_dictionary, num_classes"
      ],
      "metadata": {
        "id": "hftiXGWaJNeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_input_and_labels(without_labels, labels, max_sentence_length):\n",
        "  padded, padded_labels = pad_without_labels_and_labels(without_labels, labels, max_sentence_length)\n",
        "  padded_np, padded_labels_np = convert_padded_and_padded_labels_to_np_arrays(padded, padded_labels)\n",
        "\n",
        "  return padded_np, padded_labels_np"
      ],
      "metadata": {
        "id": "FGD4zKKiXl3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unpadded_train, unpadded_train_labels, tag_to_integer_dictionary, num_classes = process_raw_to_input_and_labels(train_raw, tag_to_integer_dictionary)\n",
        "unpadded_val, unpadded_val_labels, unused_val_dictionary, unused_val_num_classes = process_raw_to_input_and_labels(train_raw, tag_to_integer_dictionary)\n",
        "unpadded_test, unpadded_test_labels, unused_test_dictionary, unused_test_num_classes = process_raw_to_input_and_labels(test_raw, tag_to_integer_dictionary)\n",
        "\n",
        "max_sentence_length = 0\n",
        "for sentence in unpadded_train:\n",
        "  if len(sentence) > max_sentence_length:\n",
        "    max_sentence_length = len(sentence)\n",
        "for sentence in unpadded_val:\n",
        "  if len(sentence) > max_sentence_length:\n",
        "    max_sentence_length = len(sentence)\n",
        "for sentence in unpadded_test:\n",
        "  if len(sentence) > max_sentence_length:\n",
        "    max_sentence_length = len(sentence)\n",
        "\n",
        "train, train_labels = pad_input_and_labels(unpadded_train, unpadded_train_labels, max_sentence_length)\n",
        "val, val_labels = pad_input_and_labels(unpadded_val, unpadded_val_labels, max_sentence_length)\n",
        "test, test_labels = pad_input_and_labels(unpadded_test, unpadded_test_labels, max_sentence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhmf0nXISMzG",
        "outputId": "4f6a39ff-649a-4e60-a714-753d307b8a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding:  32\n",
            "padding:  32\n",
            "padding:  32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(train).any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUdj6FxMke7j",
        "outputId": "d1072a03-48bf-402b-994b-4ce2c24bea39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "id": "QRm4-c-QTJsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8a8a0c-0481-4a60-a46e-eeda9cbb4b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(826, 32, 300)\n",
            "(941, 32, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.2(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003. Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO, etc.) you chose."
      ],
      "metadata": {
        "id": "SKMMgdYn7_en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of Sentences in train: {train.shape[0]}\")\n",
        "print(f\"Number of Sentences in development: {val.shape[0]}\")\n",
        "print(f\"Number of Sentences in test: {test.shape[0]}\")\n",
        "print(f\"Possible word labels:\", end=\" \")\n",
        "for key in tag_to_integer_dictionary:\n",
        "  print(key, end=\", \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4UQO7rDOhi4",
        "outputId": "3c8a3a57-4137-4663-935c-bf77716e7ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sentences in train: 826\n",
            "Number of Sentences in development: 826\n",
            "Number of Sentences in test: 941\n",
            "Possible word labels: I-ORG, O, I-MISC, I-PER, I-LOC, B-LOC, B-MISC, B-ORG, "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.2(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word. Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."
      ],
      "metadata": {
        "id": "x37pdnww7HUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Sentence\n",
        "\"\"\"\n",
        "Germany I-LOC\n",
        "'s O\n",
        "representative O\n",
        "to O\n",
        "the O\n",
        "European I-ORG\n",
        "Union I-ORG\n",
        "'s O\n",
        "veterinary O\n",
        "committee O\n",
        "Werner I-PER\n",
        "Zwingmann I-PER\n",
        "said O\n",
        "on O\n",
        "Wednesday O\n",
        "consumers O\n",
        "should O\n",
        "buy O\n",
        "sheepmeat O\n",
        "from O\n",
        "countries O\n",
        "other O\n",
        "than O\n",
        "Britain I-LOC\n",
        "until O\n",
        "the O\n",
        "scientific O\n",
        "advice O\n",
        "was O\n",
        "clearer O\n",
        ". O\n",
        "\"\"\"\n",
        "# When t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "h-TAPmoe7GPk",
        "outputId": "cc19d635-79aa-439e-fcbe-bd3ff00be444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nGermany I-LOC\\n's O\\nrepresentative O\\nto O\\nthe O\\nEuropean I-ORG\\nUnion I-ORG\\n's O\\nveterinary O\\ncommittee O\\nWerner I-PER\\nZwingmann I-PER\\nsaid O\\non O\\nWednesday O\\nconsumers O\\nshould O\\nbuy O\\nsheepmeat O\\nfrom O\\ncountries O\\nother O\\nthan O\\nBritain I-LOC\\nuntil O\\nthe O\\nscientific O\\nadvice O\\nwas O\\nclearer O\\n. O\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct model"
      ],
      "metadata": {
        "id": "tR_aKDqO6gXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "ERqaWisbLvhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape = (train.shape[1], train.shape[2]))\n",
        "x = layers.Masking(mask_value=0.0)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(16, return_sequences=True))(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0EI7b51SKQOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6deb2f-e773-4cd1-c08b-1f4330cd7e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 300)]         0         \n",
            "                                                                 \n",
            " masking (Masking)           (None, 32, 300)           0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 32, 32)            40576     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32, 8)             264       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40840 (159.53 KB)\n",
            "Trainable params: 40840 (159.53 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self, train_data, train_labels, val_data, val_labels, mask_value=999):\n",
        "        super(F1ScoreCallback, self).__init__()\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "        self.val_data = val_data\n",
        "        self.val_labels = val_labels\n",
        "        self.mask_value = mask_value\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # Predictions for the training set\n",
        "        train_softmaxed_outputs = self.model.predict(self.train_data)\n",
        "        train_predicted_indices = np.argmax(train_softmaxed_outputs, axis=-1)\n",
        "\n",
        "        # Mask the training predictions and labels\n",
        "        train_mask = (self.train_labels != self.mask_value)\n",
        "        filtered_train_pred = train_predicted_indices[train_mask]\n",
        "        filtered_train_true = self.train_labels[train_mask]\n",
        "\n",
        "        # Predictions for the validation set\n",
        "        val_softmaxed_outputs = self.model.predict(self.val_data)\n",
        "        val_predicted_indices = np.argmax(val_softmaxed_outputs, axis=-1)\n",
        "\n",
        "        # Mask the validation predictions and labels\n",
        "        val_mask = (self.val_labels != self.mask_value)\n",
        "        filtered_val_pred = val_predicted_indices[val_mask]\n",
        "        filtered_val_true = self.val_labels[val_mask]\n",
        "\n",
        "        # Calculate the metrics\n",
        "        train_precision = precision_score(filtered_train_true, filtered_train_pred, average='weighted', labels=np.unique(filtered_train_pred))\n",
        "        train_recall = recall_score(filtered_train_true, filtered_train_pred, average='weighted', labels=np.unique(filtered_train_pred))\n",
        "        train_f1 = f1_score(filtered_train_true, filtered_train_pred, average='weighted', labels=np.unique(filtered_train_pred))\n",
        "\n",
        "        val_precision = precision_score(filtered_val_true, filtered_val_pred, average='weighted', labels=np.unique(filtered_val_pred))\n",
        "        val_recall = recall_score(filtered_val_true, filtered_val_pred, average='weighted', labels=np.unique(filtered_val_pred))\n",
        "        val_f1 = f1_score(filtered_val_true, filtered_val_pred, average='weighted', labels=np.unique(filtered_val_pred))\n",
        "\n",
        "        # Print the metrics\n",
        "        print(f'\\nEpoch {epoch + 1}')\n",
        "        print(f'Training Precision: {train_precision:.4f} | Training Recall: {train_recall:.4f} | Training F1: {train_f1:.4f}')\n",
        "        print(f'Validation Precision: {val_precision:.4f} | Validation Recall: {val_recall:.4f} | Validation F1: {val_f1:.4f}')\n",
        "\n",
        "# Then, create an instance of the F1ScoreCallback\n",
        "f1_score_callback = F1ScoreCallback(train_data=train, train_labels=train_labels, val_data=val, val_labels=val_labels)"
      ],
      "metadata": {
        "id": "ZhC-ultLUAoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss_function(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  mask = tf.cast(tf.not_equal(y_true, 999), tf.float32)\n",
        "  tf.print(\"y_true: \", y_true)\n",
        "  tf.print(\"mask: \", mask)\n",
        "  #tf.print(y_true)\n",
        "  tf.print(y_pred[0][0][:])\n",
        "  tf.print(len(y_pred))\n",
        "  tf.print(len(y_pred[0]))\n",
        "  tf.print(len(y_pred[0][0]))\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  loss *= mask\n",
        "  #tf.print(tf.reduce_sum(loss) / tf.reduce_sum(mask))\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "  \"\"\"\n",
        "  # Create a mask to ignore the loss for 999 values in y_true\n",
        "  mask = tf.cast(tf.not_equal(y_true, 999), tf.float32)\n",
        "\n",
        "  # Replace the 999 values with a valid class index (e.g., 0)\n",
        "  y_true_masked = tf.where(tf.not_equal(y_true, 999), y_true, 0)\n",
        "\n",
        "  # Calculate the loss using the modified y_true\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked, y_pred)\n",
        "\n",
        "  # Apply the mask to zero-out the loss for originally masked values\n",
        "  loss *= mask\n",
        "\n",
        "  # Return the mean loss only for the unmasked elements\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "VQmnPcFOHqV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=0.5)"
      ],
      "metadata": {
        "id": "A3orCkEhmP99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=adam_optimizer, loss=masked_loss_function, metrics=[\"accuracy\"])\n",
        "#model.fit(train, train_labels, batch_size=32, epochs=2)\n",
        "model.fit(train, train_labels, batch_size=32, epochs=10, validation_data=(val, val_labels), callbacks=[f1_score_callback])"
      ],
      "metadata": {
        "id": "GHREot04KUGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2c6e37-735d-41d3-f235-191fc73fe893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "26/26 [==============================] - 3s 24ms/step\n",
            "26/26 [==============================] - 1s 24ms/step\n",
            "\n",
            "Epoch 1\n",
            "Training Precision: 0.7772 | Training Recall: 1.0000 | Training F1: 0.8746\n",
            "Validation Precision: 0.7772 | Validation Recall: 1.0000 | Validation F1: 0.8746\n",
            "26/26 [==============================] - 31s 473ms/step - loss: 1.8791 - accuracy: 0.6211 - val_loss: 1.6248 - val_accuracy: 0.6984\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "26/26 [==============================] - 0s 17ms/step\n",
            "\n",
            "Epoch 2\n",
            "Training Precision: 0.7772 | Training Recall: 1.0000 | Training F1: 0.8746\n",
            "Validation Precision: 0.7772 | Validation Recall: 1.0000 | Validation F1: 0.8746\n",
            "26/26 [==============================] - 6s 226ms/step - loss: 1.4199 - accuracy: 0.6984 - val_loss: 1.4015 - val_accuracy: 0.6984\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - 1s 19ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 3\n",
            "Training Precision: 0.7882 | Training Recall: 0.8996 | Training F1: 0.7915\n",
            "Validation Precision: 0.7882 | Validation Recall: 0.8996 | Validation F1: 0.7915\n",
            "26/26 [==============================] - 5s 213ms/step - loss: 1.3030 - accuracy: 0.7001 - val_loss: 1.3193 - val_accuracy: 0.7023\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "26/26 [==============================] - 1s 19ms/step\n",
            "\n",
            "Epoch 4\n",
            "Training Precision: 0.8009 | Training Recall: 0.9114 | Training F1: 0.8162\n",
            "Validation Precision: 0.8009 | Validation Recall: 0.9114 | Validation F1: 0.8162\n",
            "26/26 [==============================] - 7s 266ms/step - loss: 1.2352 - accuracy: 0.7083 - val_loss: 1.2506 - val_accuracy: 0.7170\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 5\n",
            "Training Precision: 0.8219 | Training Recall: 0.9411 | Training F1: 0.8639\n",
            "Validation Precision: 0.8219 | Validation Recall: 0.9411 | Validation F1: 0.8639\n",
            "26/26 [==============================] - 5s 208ms/step - loss: 1.1718 - accuracy: 0.7344 - val_loss: 1.1863 - val_accuracy: 0.7543\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 0s 17ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 6\n",
            "Training Precision: 0.8446 | Training Recall: 0.8980 | Training F1: 0.8314\n",
            "Validation Precision: 0.8446 | Validation Recall: 0.8980 | Validation F1: 0.8314\n",
            "26/26 [==============================] - 7s 277ms/step - loss: 1.1069 - accuracy: 0.7658 - val_loss: 1.1247 - val_accuracy: 0.7815\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 7\n",
            "Training Precision: 0.8486 | Training Recall: 0.9066 | Training F1: 0.8446\n",
            "Validation Precision: 0.8486 | Validation Recall: 0.9066 | Validation F1: 0.8446\n",
            "26/26 [==============================] - 5s 211ms/step - loss: 1.0532 - accuracy: 0.7870 - val_loss: 1.0669 - val_accuracy: 0.7931\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 1s 25ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 8\n",
            "Training Precision: 0.8478 | Training Recall: 0.9169 | Training F1: 0.8639\n",
            "Validation Precision: 0.8478 | Validation Recall: 0.9169 | Validation F1: 0.8639\n",
            "26/26 [==============================] - 7s 283ms/step - loss: 1.0045 - accuracy: 0.7979 - val_loss: 1.0125 - val_accuracy: 0.8068\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "26/26 [==============================] - 0s 17ms/step\n",
            "\n",
            "Epoch 9\n",
            "Training Precision: 0.8697 | Training Recall: 0.8946 | Training F1: 0.8532\n",
            "Validation Precision: 0.8697 | Validation Recall: 0.8946 | Validation F1: 0.8532\n",
            "26/26 [==============================] - 7s 271ms/step - loss: 0.9497 - accuracy: 0.8207 - val_loss: 0.9628 - val_accuracy: 0.8317\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 1s 21ms/step\n",
            "26/26 [==============================] - 0s 18ms/step\n",
            "\n",
            "Epoch 10\n",
            "Training Precision: 0.8861 | Training Recall: 0.9107 | Training F1: 0.8742\n",
            "Validation Precision: 0.8861 | Validation Recall: 0.9107 | Validation F1: 0.8742\n",
            "26/26 [==============================] - 7s 265ms/step - loss: 0.8986 - accuracy: 0.8414 - val_loss: 0.9183 - val_accuracy: 0.8542\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792c60256f50>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing layer outputs\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Create a new model that will return the outputs from all layers:\n",
        "layer_outputs = [layer.output for layer in model.layers]  # Exclude the Input layer if necessary\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "# Get the outputs for an input:\n",
        "all_layer_activations = activation_model.predict(train)\n",
        "\n",
        "# Now iterate over the outputs and check for NaNs:\n",
        "for layer_activation in all_layer_activations:\n",
        "    # Check if the activation contains NaNs\n",
        "    if np.isnan(layer_activation).any():\n",
        "        print(\"NaNs detected\")\n",
        "\n",
        "# If you want to check a particular layer by name, you can do:\n",
        "for layer, activation in zip(model.layers, all_layer_activations):\n",
        "    if np.isnan(activation).any():\n",
        "        print(f\"NaN detected in layer: {layer.name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR1bF-mJQzYN",
        "outputId": "cfcebc84-634c-4e61-a089-2c255904a46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 3s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_softmaxed_outputs = model.predict(test)"
      ],
      "metadata": {
        "id": "XyvUIuOHXi9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddab0ee1-a279-4543-c2f1-b3b75a5e34d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 1s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((test_labels[0]))\n",
        "print((test_softmaxed_outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--xTh7vB8ltY",
        "outputId": "36aed637-e8ac-4193-fdfc-80f1fdcaf60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1   1   4   1   1   1   1   3   1   1   1   1 999 999 999 999 999 999\n",
            " 999 999 999 999 999 999 999 999 999 999 999 999 999 999]\n",
            "[[6.77952170e-02 7.18678176e-01 5.77540472e-02 6.40887842e-02\n",
            "  7.50453919e-02 4.02428024e-03 4.70536947e-03 7.90871773e-03]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [7.25751817e-02 6.33530796e-01 9.14234146e-02 3.71440798e-02\n",
            "  1.50167182e-01 3.44907516e-03 3.81101016e-03 7.89929740e-03]\n",
            " [1.32251373e-02 9.47483242e-01 1.19246030e-02 5.75576583e-03\n",
            "  1.90102775e-02 5.18397137e-04 7.81384064e-04 1.30122853e-03]\n",
            " [1.57218892e-02 9.27361488e-01 1.91534255e-02 8.04603286e-03\n",
            "  2.75467262e-02 5.21303911e-04 6.23351079e-04 1.02580211e-03]\n",
            " [1.67402029e-02 9.25362170e-01 1.94285288e-02 7.02356873e-03\n",
            "  2.93709766e-02 5.30447694e-04 5.89191623e-04 9.54823219e-04]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [3.91974039e-02 7.67323732e-01 5.90663925e-02 1.14346547e-02\n",
            "  1.17489234e-01 1.50969101e-03 1.33238418e-03 2.64646159e-03]\n",
            " [9.79339611e-03 9.56271529e-01 1.04101729e-02 2.76194559e-03\n",
            "  1.88584793e-02 5.08710276e-04 5.65227761e-04 8.30506149e-04]\n",
            " [1.38522899e-02 9.32192028e-01 1.79929007e-02 4.42118384e-03\n",
            "  2.79025175e-02 1.10299198e-03 1.09951932e-03 1.43654854e-03]\n",
            " [2.16077063e-02 8.92680109e-01 2.95882970e-02 7.26776430e-03\n",
            "  4.15525772e-02 2.53865076e-03 2.22355500e-03 2.54135416e-03]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]\n",
            " [1.18871830e-01 1.74318880e-01 1.16742827e-01 1.19406223e-01\n",
            "  1.18349358e-01 1.17174484e-01 1.17637008e-01 1.17499359e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "v7tTjR8Y_DGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdfG1Do6EN30",
        "outputId": "59313822-7f91-4ffb-baf7-dc8219cca009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_label = {idx: label for label, idx in tag_to_integer_dictionary.items()}\n",
        "predicted_indices = np.argmax(test_softmaxed_outputs, axis=-1)"
      ],
      "metadata": {
        "id": "Uf0SEsgUYub5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming predicted_indices and test_labels are numpy arrays\n",
        "predicted_indices = np.array(predicted_indices)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Flatten the arrays\n",
        "predicted_flat = predicted_indices.flatten()\n",
        "labels_flat = test_labels.flatten()\n",
        "\n",
        "# Filter out the padding values (999)\n",
        "mask = labels_flat != 999\n",
        "filtered_predictions = predicted_flat[mask]\n",
        "filtered_true_labels = labels_flat[mask]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(filtered_true_labels, filtered_predictions, average='weighted')\n",
        "recall = recall_score(filtered_true_labels, filtered_predictions, average='weighted')\n",
        "f1 = f1_score(filtered_true_labels, filtered_predictions, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4UW_7ZIQKs2",
        "outputId": "32576f91-e08e-4006-f535-780dcacefff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7757747382874197\n",
            "Recall: 0.8582400702216371\n",
            "F1 Score: 0.812918042114928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_label_predictions = [idx_to_label[index] for index in filtered_predictions]\n",
        "filtered_label_actual = [idx_to_label[index] for index in filtered_true_labels]\n",
        "for i in range(50):\n",
        "  print(\"pred: \", filtered_label_predictions[i], \" actual: \", filtered_label_actual[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt6Wt-Q4LTpF",
        "outputId": "099a2091-2002-412e-97fe-c4724222c8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  I-PER\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  I-PER  actual:  I-PER\n",
            "pred:  I-PER  actual:  I-PER\n",
            "pred:  O  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  I-PER  actual:  I-LOC\n",
            "pred:  I-LOC  actual:  I-LOC\n",
            "pred:  I-LOC  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  I-LOC  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  I-MISC\n",
            "pred:  O  actual:  I-MISC\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  I-LOC  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  I-LOC  actual:  I-LOC\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n",
            "pred:  O  actual:  O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "issues:\n",
        "1. cant use timesteps that are too high as nan values would surface\n",
        "2. cant use all the data as theres not enough ram\n",
        "3. problems visualising the code"
      ],
      "metadata": {
        "id": "QeKHzCBbGXn-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "123df16e-08b8-45d8-9a9f-4b69ca9b4c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-2b9a8a990398>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    issues:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.3(a) Discuss how you deal with new words in the training set which are not found in the pretrained dictionary. Likewise, how do you deal with new words in the test set which are not found in either the pretrained dictionary or the training set? Show the corresponding code snippet"
      ],
      "metadata": {
        "id": "2NOnh5-MAXk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For all new words not found in the pre-trained dictionary, a numpy array of size 300 that contains zeros is used in place of the usual array of size 300 pre-trained values\n",
        "\"\"\"\n",
        "if word not in embeddings:\n",
        "  new_embedding = np.zeros(300)\n",
        "\"\"\"\n",
        "# This method is simple but effective and widely used in NER tasks where there are missing embedding values"
      ],
      "metadata": {
        "id": "ypaOmoT4AWdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.3(b) Describe what neural network you used to produce the final vector representation of each word and what are the mathematical functions used for the forward computation (i.e., from the pretrained word vectors to the final label of each word). Give the detailed setting of the network including which parameters are being updated, what are their sizes, and what is the length of the final vector representation of each word to be fed to the softmax classifier."
      ],
      "metadata": {
        "id": "mRtZeGlSDRyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network transforms pre-trained word vectors into categorical labels through a bidirectional LSTM and dense layers.\n",
        "\n",
        "Components:\n",
        "\n",
        "Input Layer: Receives input of shape (None, maximum sentence length(words), word2vec embedding dimensions). Note the maximum sentence length is also the timesteps in this context.\n",
        "\n",
        "Masking Layer: Applies a mask to the input where any timestep with a value of zero is ignored, preventing padding from affecting the subsequent layers' computations.\n",
        "\n",
        "Bidirectional LSTM Layer: This layer utilizes two LSTM layers that process the data in both forward and reverse directions. Each LSTM layer comprises 16 units and implements the following mathematical operations:\n",
        "\n",
        "Input Gate: i_t = σ(W_i*[h_{t-1}, x_t] + b_i\n",
        "\n",
        "Forget Gate: f_t = σ(W_f*[h_{t-1}, x_t] + b_f)\n",
        "\n",
        "Output Gate: o_t = σ(W_o*[h_{t-1}, x_t] + b_o)\n",
        "\n",
        "Cell State: c_t = f_t * c_{t-1} + i_t * tanh(W_c*[h_{t-1}, x_t] + b_c)\n",
        "\n",
        "Hidden State: h_t = o_t * tanh(c_t)\n",
        "\n",
        "Where σ denotes the sigmoid function, tanh is the hyperbolic tangent activation function, W and b are the weights and biases of the respective gates, and * denotes element-wise multiplication. The bidirectional wrapper concatenates the outputs from both directions for each timestep, resulting in a 32-dimensional vector.\n",
        "\n",
        "Dense Output Layer: Applies a linear transformation followed by a softmax activation to the LSTM outputs to obtain the probability distribution over classes:\n",
        "\n",
        "Linear Transformation: z = W_d * h + b_d\n",
        "Softmax Activation: softmax(z) = exp(z_i) / Σexp(z_j) for i = 1 to num_classes\n",
        "Here, W_d and b_d are the weights and biases of the Dense layer, and h is the output from the Bidirectional LSTM.\n",
        "\n",
        "Training Details:\n",
        "\n",
        "Loss Function: Utilizes a custom masked_loss_function which computes the cross-entropy loss for unmasked timesteps while excluding the effects of timesteps with a label of 999.\n",
        "\n",
        "Optimizer: Employs the Adam optimizer with a specified learning rate and gradient clipping to prevent the adverse effects of large gradient updates.\n",
        "\n",
        "Training Process: The model is compiled and trained over a specified number of epochs with batch-based updates, where parameters are adjusted to minimize the custom loss function, with accuracy serving as the performance metric.\n",
        "\n",
        "Final Vector Representation:\n",
        "\n",
        "Before classification, each word is represented by a vector of length equal to num_classes, which is the output of the dense layer. This vector encodes the probability of each class given the context of the word as understood by the bidirectional LSTM."
      ],
      "metadata": {
        "id": "caXpO9BTFpo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.3(c) Report how many epochs you used for training, as well as the running time."
      ],
      "metadata": {
        "id": "Xr-h1w_HJ803"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs = 10"
      ],
      "metadata": {
        "id": "iOtogxVZKFVI"
      }
    }
  ]
}